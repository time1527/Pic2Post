{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/internvl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(\n",
    "    image, min_num=1, max_num=12, image_size=448, use_thumbnail=False\n",
    "):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j)\n",
    "        for n in range(min_num, max_num + 1)\n",
    "        for i in range(1, n + 1)\n",
    "        for j in range(1, n + 1)\n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size,\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(\n",
    "        image, image_size=input_size, use_thumbnail=True, max_num=max_num\n",
    "    )\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=512, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_vlm(model,tokenizer,image_list,header):\n",
    "    pixel_values_list = [load_image(image, max_num=12).to(torch.bfloat16).cuda() for image in image_list]\n",
    "    pixel_values = torch.cat(pixel_values_list, dim=0)\n",
    "    num_patches_list = [pv.size(0) for pv in pixel_values_list]\n",
    "    \n",
    "    image_placeholder = \"<image>\\n\"*len(image_list)\n",
    "    question = (\n",
    "        f\"ä»¥â€œ{header}â€ä¸ºé¢˜ï¼Œç»“åˆå›¾ç‰‡å†…å®¹{image_placeholder}ï¼Œå†™ä¸€ç¯‡å°çº¢ä¹¦æ–‡æ¡ˆã€‚\"\n",
    "    )\n",
    "    response, history = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        question,\n",
    "        generation_config,\n",
    "        num_patches_list=num_patches_list,\n",
    "        history=None,\n",
    "        return_history=True,\n",
    "    )\n",
    "    print(f\"User: {question}\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒå‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"OpenGVLab/InternVL2_5-1B\"\n",
    "model = (\n",
    "    AutoModel.from_pretrained(\n",
    "        path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    .eval()\n",
    "    .cuda()\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ä»¥â€œä»Šå¤©æ¥çœ‹å¤§ç†ŠçŒ«å•¦â€ä¸ºé¢˜ï¼Œç»“åˆå›¾ç‰‡å†…å®¹<image>\n",
      "<image>\n",
      "ï¼Œå†™ä¸€ç¯‡å°çº¢ä¹¦æ–‡æ¡ˆã€‚\n",
      "Assistant: æ ‡é¢˜ï¼šå¤§ç†ŠçŒ«çš„æ¸©é¦¨æ—¶åˆ»ï¼\n",
      "\n",
      "æ­£æ–‡ï¼š\n",
      "ä»Šå¤©ï¼Œæˆ‘æœ‰å¹¸å’Œå¤§å®¶åˆ†äº«äº†æˆ‘ä»¬å¯çˆ±çš„ç†ŠçŒ«å®å®ä»¬ã€‚ä½ ä»¬æ˜¯ä¸æ˜¯ä¹Ÿå–œæ¬¢è¿™æ ·çš„ç”»é¢å‘¢ï¼Ÿä½ ä»¬çŸ¥é“å—ï¼Ÿå¤§ç†ŠçŒ«æ˜¯å›½å®¶ä¸€çº§ä¿æŠ¤åŠ¨ç‰©ï¼Œå®ƒä»¬çš„å‡ºç°ï¼Œè®©æˆ‘ä»¬çš„ä¸–ç•Œæ›´åŠ ç¾å¥½ã€‚\n",
      "\n",
      "ğŸŒ¿ç†ŠçŒ«å®å®ä»¬åœ¨æˆ‘ä»¬èº«è¾¹ï¼Œå®ƒä»¬çš„æ¯›èŒ¸èŒ¸çš„èº«ä½“ï¼Œå¯çˆ±çš„è¡¨æƒ…ï¼Œè®©äººå¿ƒåŠ¨ã€‚ä½ ä»¬æ˜¯ä¸æ˜¯ä¹Ÿæƒ³å’Œå®ƒä»¬ä¸€èµ·ç©è€ï¼Œä¸€èµ·æ¢ç´¢è¿™ä¸ªä¸–ç•Œå‘¢ï¼Ÿä½ ä»¬çŸ¥é“å—ï¼Ÿå¤§ç†ŠçŒ«çš„æ¯›çš®æŸ”è½¯ï¼Œå®ƒä»¬çš„å‘¼å¸æ–¹å¼ç‹¬ç‰¹ï¼Œè®©æˆ‘ä»¬çš„ç”Ÿæ´»æ›´åŠ ç¾å¥½ã€‚\n",
      "\n",
      "#ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«ä¿æŠ¤ #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢\n",
      "\n",
      "å¸Œæœ›ä½ ä»¬ä¹Ÿèƒ½å’Œæˆ‘ä¸€æ ·ï¼Œå–œæ¬¢è¿™æ ·çš„ç”»é¢ï¼Œå–œæ¬¢è¿™æ ·çš„ç†ŠçŒ«å®å®ä»¬ã€‚è®©æˆ‘ä»¬ä¸€èµ·ä¿æŠ¤è¿™äº›å¯çˆ±çš„å°ç”Ÿå‘½ï¼Œè®©å®ƒä»¬åœ¨æˆ‘ä»¬çš„ä¸–ç•Œé‡Œï¼Œç»§ç»­å¿«ä¹åœ°ç”Ÿæ´»ã€‚\n",
      "\n",
      "#ç†ŠçŒ« #ç†ŠçŒ«ä¿æŠ¤ #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«ä¿æŠ¤ #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«ç”Ÿæ´» #ç†ŠçŒ«å¯çˆ± #ç†ŠçŒ«æ¢ç´¢ #ç†ŠçŒ«\n"
     ]
    }
   ],
   "source": [
    "chat_vlm(\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    image_list=[\n",
    "        # \"https://d.ifengimg.com/q100/img1.ugc.ifeng.com/newugc/20191218/12/wemedia/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        # \"https://img.pconline.com.cn/images/upload/upc/tx/itbbs/1404/29/c18/33701467_1398762909920_mthumb.jpg\"\n",
    "        \"./assets/test/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        \"./assets/test/33701467_1398762909920_mthumb.jpg\"\n",
    "                ],\n",
    "    header=\"ä»Šå¤©æ¥çœ‹å¤§ç†ŠçŒ«å•¦\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒåæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(dir_path,\"InternVL/internvl_chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 638,496,128 || trainable%: 1.3779579255334184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InternVLChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn): FlashAttention()\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151674, 896)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2Attention(\n",
       "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=896, out_features=151674, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=4096, out_features=896, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=896, out_features=896, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from internvl.model.internvl_chat import InternVLChatModel\n",
    "\n",
    "################## è¿™é‡Œå¡«å†™å¾®è°ƒåçš„loraè·¯å¾„#####################\n",
    "lora_path = os.path.join(dir_path,\"InternVL/internvl_chat/work_dirs/internvl_chat_v2_5/internvl2_5_1b_dynamic_res_2nd_finetune_lora\")\n",
    "model = InternVLChatModel.from_pretrained(\n",
    "    lora_path, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)\n",
    "\n",
    "if model.config.use_backbone_lora:\n",
    "    model.vision_model.merge_and_unload()\n",
    "    model.vision_model = model.vision_model.model\n",
    "    model.config.use_backbone_lora = 0\n",
    "if model.config.use_llm_lora:\n",
    "    model.language_model.merge_and_unload()\n",
    "    model.language_model = model.language_model.model\n",
    "    model.config.use_llm_lora = 0\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ä»¥â€œä»Šå¤©æ¥çœ‹å¤§ç†ŠçŒ«å•¦â€ä¸ºé¢˜ï¼Œç»“åˆå›¾ç‰‡å†…å®¹<image>\n",
      "<image>\n",
      "ï¼Œå†™ä¸€ç¯‡å°çº¢ä¹¦æ–‡æ¡ˆã€‚\n",
      "Assistant: æ ‡é¢˜ï¼šä»Šå¤©æ¥çœ‹å¤§ç†ŠçŒ«å•¦ï¼ğŸŒ¿\n",
      "\n",
      "æ–‡æ¡ˆï¼š\n",
      "\n",
      "å¤§ç†ŠçŒ«ï¼Œæ†¨æ€å¯æ¬çš„åŠ¨ç‰©ï¼Œæ˜¯ä¸æ˜¯ä¹Ÿè®©ä½ å¿ƒåŠ¨äº†ï¼Ÿä»Šå¤©å¸¦å¤§å®¶èµ°è¿›å®ƒä»¬çš„è‡ªç„¶æ –æ¯åœ°ï¼Œæ„Ÿå—ä¸€ä¸‹å¤§ç†ŠçŒ«çš„æ—¥å¸¸ç”Ÿæ´»ï¼\n",
      "\n",
      "ğŸŒ¿ **è‡ªç„¶æ –æ¯åœ°**  \n",
      "å¤§ç†ŠçŒ«å–œæ¬¢åœ¨æ£®æ—é‡Œæ´»åŠ¨ï¼Œå®ƒä»¬çš„å®¶æ˜¯éƒéƒè‘±è‘±çš„ç«¹æ—å’ŒèŒ‚å¯†çš„çŒæœ¨ä¸›ã€‚æ¯å¤©æ¸…æ™¨ï¼Œå®ƒä»¬ä¼šåœ¨æ ‘ä¸‹è§…é£Ÿï¼Œé˜³å…‰é€è¿‡æ ‘å¶æ´’åœ¨å®ƒä»¬èº«ä¸Šï¼Œä»¿ä½›åœ¨ä¸ºå®ƒä»¬ç¼–ç»‡ä¸€å¹…ç¾ä¸½çš„ç”»å·ã€‚åˆåï¼Œå®ƒä»¬ä¼šåœ¨æ ‘ä¸‹ä¼‘æ¯ï¼Œäº«å—ç€é˜³å…‰å’Œå¤§è‡ªç„¶çš„æ©èµã€‚\n",
      "\n",
      "ğŸ± **å¯çˆ±çš„å°è„¸**  \n",
      "å¤§ç†ŠçŒ«çš„å¯çˆ±ä¹‹å¤„å°±åœ¨äºå®ƒä»¬é‚£åœ†åœ†çš„å¤§çœ¼ç›å’ŒæŸ”è½¯çš„æ¯›å‘ã€‚å®ƒä»¬çš„é¼»å­ç‰¹åˆ«é•¿ï¼Œåƒä¸€æŠŠå¤§ä¼ï¼Œå¯ä»¥ä¼¸å‡ºæ¥å‘¼å¸æ–°é²œç©ºæ°”ã€‚å®ƒä»¬çš„è€³æœµç«–èµ·æ¥ï¼Œä»¿ä½›åœ¨å€¾å¬å‘¨å›´çš„åŠ¨é™ï¼Œä»¿ä½›åœ¨å’Œæˆ‘ä»¬è¯´æ‚„æ‚„è¯ã€‚\n",
      "\n",
      "ğŸŒ¿ **é£Ÿç‰©çš„é€‰æ‹©**  \n",
      "å¤§ç†ŠçŒ«çš„é£Ÿç‰©éå¸¸ä¸°å¯Œï¼ŒåŒ…æ‹¬ç«¹å­ã€å±±è¯ã€è˜‘è‡ç­‰ã€‚å®ƒä»¬å–œæ¬¢åœ¨æ ‘ä¸Šå¯»æ‰¾é£Ÿç‰©ï¼Œæœ‰æ—¶å€™è¿˜ä¼šåœ¨æ ‘æ´é‡ŒæŒ–æ´ï¼Œå¯»æ‰¾éšè—çš„ç¾å‘³ã€‚å®ƒä»¬çš„é¥®é£Ÿä¹ æƒ¯éå¸¸ç‹¬ç‰¹ï¼Œå¯ä»¥è¯´æ˜¯â€œæ‚é£ŸåŠ¨ç‰©â€ï¼\n",
      "\n",
      "å¤§ç†ŠçŒ«ä¸ä»…å¯çˆ±ï¼Œè¿˜éå¸¸èªæ˜ï¼Œå®ƒä»¬ä¼šç”¨é¼»å­æ¥å—…æ¢é£Ÿç‰©ï¼Œè¿˜ä¼šç”¨å˜´å·´æ¥å’¬é£Ÿç‰©ã€‚å®ƒä»¬çš„æ™ºæ…§å’Œè€å¿ƒï¼Œè®©äººæƒŠå¹ä¸å·²ã€‚\n",
      "\n",
      "å¤§ç†ŠçŒ«ï¼Œä½ ä»¬æ˜¯å¤§è‡ªç„¶çš„å®ˆæŠ¤è€…ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬çš„æœ‹å‹ã€‚ä¸‹æ¬¡å¦‚æœæœ‰æœºä¼šï¼Œä¸€å®šè¦å»å®ƒä»¬çš„æ –æ¯åœ°çœ‹çœ‹ï¼Œæ„Ÿå—ä¸€ä¸‹å®ƒä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼ğŸŒ¿\n",
      "\n",
      "#å¤§ç†ŠçŒ« #ç†ŠçŒ« #è‡ªç„¶æ –æ¯åœ° #ç†ŠçŒ«ç”Ÿæ´» #è‡ªç„¶è§‚å¯Ÿ #åŠ¨ç‰©æ‘„å½± #è‡ªç„¶æ‘„å½± #å¤§ç†ŠçŒ«æ‘„å½±\n"
     ]
    }
   ],
   "source": [
    "chat_vlm(\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    image_list=[\n",
    "        # \"https://d.ifengimg.com/q100/img1.ugc.ifeng.com/newugc/20191218/12/wemedia/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        # \"https://img.pconline.com.cn/images/upload/upc/tx/itbbs/1404/29/c18/33701467_1398762909920_mthumb.jpg\"\n",
    "        \"./assets/test/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        \"./assets/test/33701467_1398762909920_mthumb.jpg\"\n",
    "        ],\n",
    "    header=\"ä»Šå¤©æ¥çœ‹å¤§ç†ŠçŒ«å•¦\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
