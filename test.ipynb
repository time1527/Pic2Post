{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/internvl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(\n",
    "    image, min_num=1, max_num=12, image_size=448, use_thumbnail=False\n",
    "):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j)\n",
    "        for n in range(min_num, max_num + 1)\n",
    "        for i in range(1, n + 1)\n",
    "        for j in range(1, n + 1)\n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size,\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(\n",
    "        image, image_size=input_size, use_thumbnail=True, max_num=max_num\n",
    "    )\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=512, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_vlm(model,tokenizer,image_list,header):\n",
    "    pixel_values_list = [load_image(image, max_num=12).to(torch.bfloat16).cuda() for image in image_list]\n",
    "    pixel_values = torch.cat(pixel_values_list, dim=0)\n",
    "    num_patches_list = [pv.size(0) for pv in pixel_values_list]\n",
    "    \n",
    "    image_placeholder = \"<image>\\n\"*len(image_list)\n",
    "    question = (\n",
    "        f\"以“{header}”为题，结合图片内容{image_placeholder}，写一篇小红书文案。\"\n",
    "    )\n",
    "    response, history = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        question,\n",
    "        generation_config,\n",
    "        num_patches_list=num_patches_list,\n",
    "        history=None,\n",
    "        return_history=True,\n",
    "    )\n",
    "    print(f\"User: {question}\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调前模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"OpenGVLab/InternVL2_5-1B\"\n",
    "model = (\n",
    "    AutoModel.from_pretrained(\n",
    "        path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    .eval()\n",
    "    .cuda()\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 以“今天来看大熊猫啦”为题，结合图片内容<image>\n",
      "<image>\n",
      "，写一篇小红书文案。\n",
      "Assistant: 标题：大熊猫的温馨时刻！\n",
      "\n",
      "正文：\n",
      "今天，我有幸和大家分享了我们可爱的熊猫宝宝们。你们是不是也喜欢这样的画面呢？你们知道吗？大熊猫是国家一级保护动物，它们的出现，让我们的世界更加美好。\n",
      "\n",
      "🌿熊猫宝宝们在我们身边，它们的毛茸茸的身体，可爱的表情，让人心动。你们是不是也想和它们一起玩耍，一起探索这个世界呢？你们知道吗？大熊猫的毛皮柔软，它们的呼吸方式独特，让我们的生活更加美好。\n",
      "\n",
      "#熊猫生活 #熊猫保护 #熊猫可爱 #熊猫探索\n",
      "\n",
      "希望你们也能和我一样，喜欢这样的画面，喜欢这样的熊猫宝宝们。让我们一起保护这些可爱的小生命，让它们在我们的世界里，继续快乐地生活。\n",
      "\n",
      "#熊猫 #熊猫保护 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫保护 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫生活 #熊猫可爱 #熊猫探索 #熊猫\n"
     ]
    }
   ],
   "source": [
    "chat_vlm(\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    image_list=[\n",
    "        # \"https://d.ifengimg.com/q100/img1.ugc.ifeng.com/newugc/20191218/12/wemedia/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        # \"https://img.pconline.com.cn/images/upload/upc/tx/itbbs/1404/29/c18/33701467_1398762909920_mthumb.jpg\"\n",
    "        \"./assets/test/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        \"./assets/test/33701467_1398762909920_mthumb.jpg\"\n",
    "                ],\n",
    "    header=\"今天来看大熊猫啦\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调后模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(dir_path,\"InternVL/internvl_chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 638,496,128 || trainable%: 1.3779579255334184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InternVLChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn): FlashAttention()\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151674, 896)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2Attention(\n",
       "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=896, out_features=151674, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=4096, out_features=896, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=896, out_features=896, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from internvl.model.internvl_chat import InternVLChatModel\n",
    "\n",
    "################## 这里填写微调后的lora路径#####################\n",
    "lora_path = os.path.join(dir_path,\"InternVL/internvl_chat/work_dirs/internvl_chat_v2_5/internvl2_5_1b_dynamic_res_2nd_finetune_lora\")\n",
    "model = InternVLChatModel.from_pretrained(\n",
    "    lora_path, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)\n",
    "\n",
    "if model.config.use_backbone_lora:\n",
    "    model.vision_model.merge_and_unload()\n",
    "    model.vision_model = model.vision_model.model\n",
    "    model.config.use_backbone_lora = 0\n",
    "if model.config.use_llm_lora:\n",
    "    model.language_model.merge_and_unload()\n",
    "    model.language_model = model.language_model.model\n",
    "    model.config.use_llm_lora = 0\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 以“今天来看大熊猫啦”为题，结合图片内容<image>\n",
      "<image>\n",
      "，写一篇小红书文案。\n",
      "Assistant: 标题：今天来看大熊猫啦！🌿\n",
      "\n",
      "文案：\n",
      "\n",
      "大熊猫，憨态可掬的动物，是不是也让你心动了？今天带大家走进它们的自然栖息地，感受一下大熊猫的日常生活！\n",
      "\n",
      "🌿 **自然栖息地**  \n",
      "大熊猫喜欢在森林里活动，它们的家是郁郁葱葱的竹林和茂密的灌木丛。每天清晨，它们会在树下觅食，阳光透过树叶洒在它们身上，仿佛在为它们编织一幅美丽的画卷。午后，它们会在树下休息，享受着阳光和大自然的恩赐。\n",
      "\n",
      "🐱 **可爱的小脸**  \n",
      "大熊猫的可爱之处就在于它们那圆圆的大眼睛和柔软的毛发。它们的鼻子特别长，像一把大伞，可以伸出来呼吸新鲜空气。它们的耳朵竖起来，仿佛在倾听周围的动静，仿佛在和我们说悄悄话。\n",
      "\n",
      "🌿 **食物的选择**  \n",
      "大熊猫的食物非常丰富，包括竹子、山药、蘑菇等。它们喜欢在树上寻找食物，有时候还会在树洞里挖洞，寻找隐藏的美味。它们的饮食习惯非常独特，可以说是“杂食动物”！\n",
      "\n",
      "大熊猫不仅可爱，还非常聪明，它们会用鼻子来嗅探食物，还会用嘴巴来咬食物。它们的智慧和耐心，让人惊叹不已。\n",
      "\n",
      "大熊猫，你们是大自然的守护者，也是我们的朋友。下次如果有机会，一定要去它们的栖息地看看，感受一下它们的日常生活！🌿\n",
      "\n",
      "#大熊猫 #熊猫 #自然栖息地 #熊猫生活 #自然观察 #动物摄影 #自然摄影 #大熊猫摄影\n"
     ]
    }
   ],
   "source": [
    "chat_vlm(\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    image_list=[\n",
    "        # \"https://d.ifengimg.com/q100/img1.ugc.ifeng.com/newugc/20191218/12/wemedia/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        # \"https://img.pconline.com.cn/images/upload/upc/tx/itbbs/1404/29/c18/33701467_1398762909920_mthumb.jpg\"\n",
    "        \"./assets/test/cdb8eda8463e7728ae2ccfbd0273292c947ec322_size885_w1620_h1080.jpg\",\n",
    "        \"./assets/test/33701467_1398762909920_mthumb.jpg\"\n",
    "        ],\n",
    "    header=\"今天来看大熊猫啦\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
